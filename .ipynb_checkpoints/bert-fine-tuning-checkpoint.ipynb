{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/data/sentimentanalysis/train_df.csv')\n",
    "df_test = pd.read_csv('/data/sentimentanalysis/test_df.csv')\n",
    "df_val = pd.read_csv('/data/sentimentanalysis/valid_df.csv', )\n",
    "\n",
    "df = pd.concat([df_train, df_test, df_val], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a67127f77c403187c8a34629d4be05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 467 ms, total: 3min 58s\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_sentences = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in df.Review:\n",
    "    input_sen = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=tokenizer.model_max_length,\n",
    "                                      pad_to_max_length=True, return_tensors='pt', return_attention_mask=True)\n",
    "    \n",
    "    input_sentences.append(input_sen['input_ids'])\n",
    "    attention_masks.append(input_sen['attention_mask'])\n",
    "    \n",
    "    \n",
    "labels = torch.tensor(df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "input_sentences = torch.cat(input_sentences, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17,500 training samples\n",
      "3,750 test samples\n",
      "3,750 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_sentences, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.7*len(dataset))\n",
    "test_size = int(0.15*len(dataset))\n",
    "valid_size = len(dataset) - train_size - test_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "print('{} training samples'.format(train_size))\n",
    "print('{} test samples'.format(test_size))\n",
    "print('{} validation samples'.format(valid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset),\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "valid_loader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset),\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset),\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb1af62a0b5416fba4f419dfe284d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b66487f89f8459c8e2759a77e67a770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  3,500.    Elapsed: 0:00:14.\n",
      "  Batch    80  of  3,500.    Elapsed: 0:00:27.\n",
      "  Batch   120  of  3,500.    Elapsed: 0:00:40.\n",
      "  Batch   160  of  3,500.    Elapsed: 0:00:53.\n",
      "  Batch   200  of  3,500.    Elapsed: 0:01:06.\n",
      "  Batch   240  of  3,500.    Elapsed: 0:01:19.\n",
      "  Batch   280  of  3,500.    Elapsed: 0:01:32.\n",
      "  Batch   320  of  3,500.    Elapsed: 0:01:46.\n",
      "  Batch   360  of  3,500.    Elapsed: 0:01:59.\n",
      "  Batch   400  of  3,500.    Elapsed: 0:02:12.\n",
      "  Batch   440  of  3,500.    Elapsed: 0:02:25.\n",
      "  Batch   480  of  3,500.    Elapsed: 0:02:38.\n",
      "  Batch   520  of  3,500.    Elapsed: 0:02:51.\n",
      "  Batch   560  of  3,500.    Elapsed: 0:03:04.\n",
      "  Batch   600  of  3,500.    Elapsed: 0:03:17.\n",
      "  Batch   640  of  3,500.    Elapsed: 0:03:30.\n",
      "  Batch   680  of  3,500.    Elapsed: 0:03:43.\n",
      "  Batch   720  of  3,500.    Elapsed: 0:03:56.\n",
      "  Batch   760  of  3,500.    Elapsed: 0:04:09.\n",
      "  Batch   800  of  3,500.    Elapsed: 0:04:22.\n",
      "  Batch   840  of  3,500.    Elapsed: 0:04:35.\n",
      "  Batch   880  of  3,500.    Elapsed: 0:04:48.\n",
      "  Batch   920  of  3,500.    Elapsed: 0:05:01.\n",
      "  Batch   960  of  3,500.    Elapsed: 0:05:14.\n",
      "  Batch 1,000  of  3,500.    Elapsed: 0:05:27.\n",
      "  Batch 1,040  of  3,500.    Elapsed: 0:05:40.\n",
      "  Batch 1,080  of  3,500.    Elapsed: 0:05:53.\n",
      "  Batch 1,120  of  3,500.    Elapsed: 0:06:06.\n",
      "  Batch 1,160  of  3,500.    Elapsed: 0:06:19.\n",
      "  Batch 1,200  of  3,500.    Elapsed: 0:06:32.\n",
      "  Batch 1,240  of  3,500.    Elapsed: 0:06:45.\n",
      "  Batch 1,280  of  3,500.    Elapsed: 0:06:59.\n",
      "  Batch 1,320  of  3,500.    Elapsed: 0:07:12.\n",
      "  Batch 1,360  of  3,500.    Elapsed: 0:07:25.\n",
      "  Batch 1,400  of  3,500.    Elapsed: 0:07:38.\n",
      "  Batch 1,440  of  3,500.    Elapsed: 0:07:51.\n",
      "  Batch 1,480  of  3,500.    Elapsed: 0:08:04.\n",
      "  Batch 1,520  of  3,500.    Elapsed: 0:08:17.\n",
      "  Batch 1,560  of  3,500.    Elapsed: 0:08:30.\n",
      "  Batch 1,600  of  3,500.    Elapsed: 0:08:43.\n",
      "  Batch 1,640  of  3,500.    Elapsed: 0:08:56.\n",
      "  Batch 1,680  of  3,500.    Elapsed: 0:09:09.\n",
      "  Batch 1,720  of  3,500.    Elapsed: 0:09:22.\n",
      "  Batch 1,760  of  3,500.    Elapsed: 0:09:35.\n",
      "  Batch 1,800  of  3,500.    Elapsed: 0:09:48.\n",
      "  Batch 1,840  of  3,500.    Elapsed: 0:10:01.\n",
      "  Batch 1,880  of  3,500.    Elapsed: 0:10:14.\n",
      "  Batch 1,920  of  3,500.    Elapsed: 0:10:27.\n",
      "  Batch 1,960  of  3,500.    Elapsed: 0:10:40.\n",
      "  Batch 2,000  of  3,500.    Elapsed: 0:10:53.\n",
      "  Batch 2,040  of  3,500.    Elapsed: 0:11:06.\n",
      "  Batch 2,080  of  3,500.    Elapsed: 0:11:19.\n",
      "  Batch 2,120  of  3,500.    Elapsed: 0:11:32.\n",
      "  Batch 2,160  of  3,500.    Elapsed: 0:11:45.\n",
      "  Batch 2,200  of  3,500.    Elapsed: 0:11:59.\n",
      "  Batch 2,240  of  3,500.    Elapsed: 0:12:12.\n",
      "  Batch 2,280  of  3,500.    Elapsed: 0:12:25.\n",
      "  Batch 2,320  of  3,500.    Elapsed: 0:12:38.\n",
      "  Batch 2,360  of  3,500.    Elapsed: 0:12:51.\n",
      "  Batch 2,400  of  3,500.    Elapsed: 0:13:04.\n",
      "  Batch 2,440  of  3,500.    Elapsed: 0:13:17.\n",
      "  Batch 2,480  of  3,500.    Elapsed: 0:13:30.\n",
      "  Batch 2,520  of  3,500.    Elapsed: 0:13:43.\n",
      "  Batch 2,560  of  3,500.    Elapsed: 0:13:56.\n",
      "  Batch 2,600  of  3,500.    Elapsed: 0:14:09.\n",
      "  Batch 2,640  of  3,500.    Elapsed: 0:14:22.\n",
      "  Batch 2,680  of  3,500.    Elapsed: 0:14:35.\n",
      "  Batch 2,720  of  3,500.    Elapsed: 0:14:48.\n",
      "  Batch 2,760  of  3,500.    Elapsed: 0:15:01.\n",
      "  Batch 2,800  of  3,500.    Elapsed: 0:15:14.\n",
      "  Batch 2,840  of  3,500.    Elapsed: 0:15:27.\n",
      "  Batch 2,880  of  3,500.    Elapsed: 0:15:40.\n",
      "  Batch 2,920  of  3,500.    Elapsed: 0:15:53.\n",
      "  Batch 2,960  of  3,500.    Elapsed: 0:16:06.\n",
      "  Batch 3,000  of  3,500.    Elapsed: 0:16:19.\n",
      "  Batch 3,040  of  3,500.    Elapsed: 0:16:32.\n",
      "  Batch 3,080  of  3,500.    Elapsed: 0:16:45.\n",
      "  Batch 3,120  of  3,500.    Elapsed: 0:16:58.\n",
      "  Batch 3,160  of  3,500.    Elapsed: 0:17:11.\n",
      "  Batch 3,200  of  3,500.    Elapsed: 0:17:24.\n",
      "  Batch 3,240  of  3,500.    Elapsed: 0:17:37.\n",
      "  Batch 3,280  of  3,500.    Elapsed: 0:17:51.\n",
      "  Batch 3,320  of  3,500.    Elapsed: 0:18:04.\n",
      "  Batch 3,360  of  3,500.    Elapsed: 0:18:17.\n",
      "  Batch 3,400  of  3,500.    Elapsed: 0:18:30.\n",
      "  Batch 3,440  of  3,500.    Elapsed: 0:18:43.\n",
      "  Batch 3,480  of  3,500.    Elapsed: 0:18:56.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epcoh took: 0:19:02\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.31\n",
      "  Validation took: 0:01:16\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  3,500.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  3,500.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  3,500.    Elapsed: 0:00:39.\n",
      "  Batch   160  of  3,500.    Elapsed: 0:00:52.\n",
      "  Batch   200  of  3,500.    Elapsed: 0:01:05.\n",
      "  Batch   240  of  3,500.    Elapsed: 0:01:18.\n",
      "  Batch   280  of  3,500.    Elapsed: 0:01:31.\n",
      "  Batch   320  of  3,500.    Elapsed: 0:01:44.\n",
      "  Batch   360  of  3,500.    Elapsed: 0:01:57.\n",
      "  Batch   400  of  3,500.    Elapsed: 0:02:10.\n",
      "  Batch   440  of  3,500.    Elapsed: 0:02:23.\n",
      "  Batch   480  of  3,500.    Elapsed: 0:02:36.\n",
      "  Batch   520  of  3,500.    Elapsed: 0:02:49.\n",
      "  Batch   560  of  3,500.    Elapsed: 0:03:02.\n",
      "  Batch   600  of  3,500.    Elapsed: 0:03:15.\n",
      "  Batch   640  of  3,500.    Elapsed: 0:03:28.\n",
      "  Batch   680  of  3,500.    Elapsed: 0:03:41.\n",
      "  Batch   720  of  3,500.    Elapsed: 0:03:54.\n",
      "  Batch   760  of  3,500.    Elapsed: 0:04:07.\n",
      "  Batch   800  of  3,500.    Elapsed: 0:04:20.\n",
      "  Batch   840  of  3,500.    Elapsed: 0:04:33.\n",
      "  Batch   880  of  3,500.    Elapsed: 0:04:46.\n",
      "  Batch   920  of  3,500.    Elapsed: 0:04:59.\n",
      "  Batch   960  of  3,500.    Elapsed: 0:05:12.\n",
      "  Batch 1,000  of  3,500.    Elapsed: 0:05:25.\n",
      "  Batch 1,040  of  3,500.    Elapsed: 0:05:38.\n",
      "  Batch 1,080  of  3,500.    Elapsed: 0:05:51.\n",
      "  Batch 1,120  of  3,500.    Elapsed: 0:06:04.\n",
      "  Batch 1,160  of  3,500.    Elapsed: 0:06:17.\n",
      "  Batch 1,200  of  3,500.    Elapsed: 0:06:30.\n",
      "  Batch 1,240  of  3,500.    Elapsed: 0:06:43.\n",
      "  Batch 1,280  of  3,500.    Elapsed: 0:06:56.\n",
      "  Batch 1,320  of  3,500.    Elapsed: 0:07:09.\n",
      "  Batch 1,360  of  3,500.    Elapsed: 0:07:22.\n",
      "  Batch 1,400  of  3,500.    Elapsed: 0:07:35.\n",
      "  Batch 1,440  of  3,500.    Elapsed: 0:07:48.\n",
      "  Batch 1,480  of  3,500.    Elapsed: 0:08:01.\n",
      "  Batch 1,520  of  3,500.    Elapsed: 0:08:14.\n",
      "  Batch 1,560  of  3,500.    Elapsed: 0:08:27.\n",
      "  Batch 1,600  of  3,500.    Elapsed: 0:08:40.\n",
      "  Batch 1,640  of  3,500.    Elapsed: 0:08:53.\n",
      "  Batch 1,680  of  3,500.    Elapsed: 0:09:06.\n",
      "  Batch 1,720  of  3,500.    Elapsed: 0:09:19.\n",
      "  Batch 1,760  of  3,500.    Elapsed: 0:09:32.\n",
      "  Batch 1,800  of  3,500.    Elapsed: 0:09:45.\n",
      "  Batch 1,840  of  3,500.    Elapsed: 0:09:58.\n",
      "  Batch 1,880  of  3,500.    Elapsed: 0:10:11.\n",
      "  Batch 1,920  of  3,500.    Elapsed: 0:10:25.\n",
      "  Batch 1,960  of  3,500.    Elapsed: 0:10:38.\n",
      "  Batch 2,000  of  3,500.    Elapsed: 0:10:51.\n",
      "  Batch 2,040  of  3,500.    Elapsed: 0:11:04.\n",
      "  Batch 2,080  of  3,500.    Elapsed: 0:11:17.\n",
      "  Batch 2,120  of  3,500.    Elapsed: 0:11:30.\n",
      "  Batch 2,160  of  3,500.    Elapsed: 0:11:43.\n",
      "  Batch 2,200  of  3,500.    Elapsed: 0:11:56.\n",
      "  Batch 2,240  of  3,500.    Elapsed: 0:12:09.\n",
      "  Batch 2,280  of  3,500.    Elapsed: 0:12:22.\n",
      "  Batch 2,320  of  3,500.    Elapsed: 0:12:35.\n",
      "  Batch 2,360  of  3,500.    Elapsed: 0:12:48.\n",
      "  Batch 2,400  of  3,500.    Elapsed: 0:13:01.\n",
      "  Batch 2,440  of  3,500.    Elapsed: 0:13:14.\n",
      "  Batch 2,480  of  3,500.    Elapsed: 0:13:27.\n",
      "  Batch 2,520  of  3,500.    Elapsed: 0:13:40.\n",
      "  Batch 2,560  of  3,500.    Elapsed: 0:13:53.\n",
      "  Batch 2,600  of  3,500.    Elapsed: 0:14:06.\n",
      "  Batch 2,640  of  3,500.    Elapsed: 0:14:19.\n",
      "  Batch 2,680  of  3,500.    Elapsed: 0:14:32.\n",
      "  Batch 2,720  of  3,500.    Elapsed: 0:14:45.\n",
      "  Batch 2,760  of  3,500.    Elapsed: 0:14:58.\n",
      "  Batch 2,800  of  3,500.    Elapsed: 0:15:11.\n",
      "  Batch 2,840  of  3,500.    Elapsed: 0:15:24.\n",
      "  Batch 2,880  of  3,500.    Elapsed: 0:15:37.\n",
      "  Batch 2,920  of  3,500.    Elapsed: 0:15:50.\n",
      "  Batch 2,960  of  3,500.    Elapsed: 0:16:03.\n",
      "  Batch 3,000  of  3,500.    Elapsed: 0:16:16.\n",
      "  Batch 3,040  of  3,500.    Elapsed: 0:16:29.\n",
      "  Batch 3,080  of  3,500.    Elapsed: 0:16:42.\n",
      "  Batch 3,120  of  3,500.    Elapsed: 0:16:55.\n",
      "  Batch 3,160  of  3,500.    Elapsed: 0:17:08.\n",
      "  Batch 3,200  of  3,500.    Elapsed: 0:17:21.\n",
      "  Batch 3,240  of  3,500.    Elapsed: 0:17:34.\n",
      "  Batch 3,280  of  3,500.    Elapsed: 0:17:47.\n",
      "  Batch 3,320  of  3,500.    Elapsed: 0:18:00.\n",
      "  Batch 3,360  of  3,500.    Elapsed: 0:18:13.\n",
      "  Batch 3,400  of  3,500.    Elapsed: 0:18:26.\n",
      "  Batch 3,440  of  3,500.    Elapsed: 0:18:39.\n",
      "  Batch 3,480  of  3,500.    Elapsed: 0:18:52.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:18:58\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:01:16\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  3,500.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  3,500.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  3,500.    Elapsed: 0:00:39.\n",
      "  Batch   160  of  3,500.    Elapsed: 0:00:52.\n",
      "  Batch   200  of  3,500.    Elapsed: 0:01:05.\n",
      "  Batch   240  of  3,500.    Elapsed: 0:01:18.\n",
      "  Batch   280  of  3,500.    Elapsed: 0:01:31.\n",
      "  Batch   320  of  3,500.    Elapsed: 0:01:44.\n",
      "  Batch   360  of  3,500.    Elapsed: 0:01:57.\n",
      "  Batch   400  of  3,500.    Elapsed: 0:02:10.\n",
      "  Batch   440  of  3,500.    Elapsed: 0:02:23.\n",
      "  Batch   480  of  3,500.    Elapsed: 0:02:36.\n",
      "  Batch   520  of  3,500.    Elapsed: 0:02:49.\n",
      "  Batch   560  of  3,500.    Elapsed: 0:03:02.\n",
      "  Batch   600  of  3,500.    Elapsed: 0:03:15.\n",
      "  Batch   640  of  3,500.    Elapsed: 0:03:28.\n",
      "  Batch   680  of  3,500.    Elapsed: 0:03:41.\n",
      "  Batch   720  of  3,500.    Elapsed: 0:03:54.\n",
      "  Batch   760  of  3,500.    Elapsed: 0:04:07.\n",
      "  Batch   800  of  3,500.    Elapsed: 0:04:20.\n",
      "  Batch   840  of  3,500.    Elapsed: 0:04:33.\n",
      "  Batch   880  of  3,500.    Elapsed: 0:04:46.\n",
      "  Batch   920  of  3,500.    Elapsed: 0:04:59.\n",
      "  Batch   960  of  3,500.    Elapsed: 0:05:12.\n",
      "  Batch 1,000  of  3,500.    Elapsed: 0:05:25.\n",
      "  Batch 1,040  of  3,500.    Elapsed: 0:05:38.\n",
      "  Batch 1,080  of  3,500.    Elapsed: 0:05:51.\n",
      "  Batch 1,120  of  3,500.    Elapsed: 0:06:04.\n",
      "  Batch 1,160  of  3,500.    Elapsed: 0:06:17.\n",
      "  Batch 1,200  of  3,500.    Elapsed: 0:06:30.\n",
      "  Batch 1,240  of  3,500.    Elapsed: 0:06:43.\n",
      "  Batch 1,280  of  3,500.    Elapsed: 0:06:56.\n",
      "  Batch 1,320  of  3,500.    Elapsed: 0:07:09.\n",
      "  Batch 1,360  of  3,500.    Elapsed: 0:07:22.\n",
      "  Batch 1,400  of  3,500.    Elapsed: 0:07:35.\n",
      "  Batch 1,440  of  3,500.    Elapsed: 0:07:48.\n",
      "  Batch 1,480  of  3,500.    Elapsed: 0:08:01.\n",
      "  Batch 1,520  of  3,500.    Elapsed: 0:08:14.\n",
      "  Batch 1,560  of  3,500.    Elapsed: 0:08:27.\n",
      "  Batch 1,600  of  3,500.    Elapsed: 0:08:40.\n",
      "  Batch 1,640  of  3,500.    Elapsed: 0:08:53.\n",
      "  Batch 1,680  of  3,500.    Elapsed: 0:09:05.\n",
      "  Batch 1,720  of  3,500.    Elapsed: 0:09:19.\n",
      "  Batch 1,760  of  3,500.    Elapsed: 0:09:32.\n",
      "  Batch 1,800  of  3,500.    Elapsed: 0:09:45.\n",
      "  Batch 1,840  of  3,500.    Elapsed: 0:09:58.\n",
      "  Batch 1,880  of  3,500.    Elapsed: 0:10:11.\n",
      "  Batch 1,920  of  3,500.    Elapsed: 0:10:24.\n",
      "  Batch 1,960  of  3,500.    Elapsed: 0:10:37.\n",
      "  Batch 2,000  of  3,500.    Elapsed: 0:10:50.\n",
      "  Batch 2,040  of  3,500.    Elapsed: 0:11:03.\n",
      "  Batch 2,080  of  3,500.    Elapsed: 0:11:16.\n",
      "  Batch 2,120  of  3,500.    Elapsed: 0:11:29.\n",
      "  Batch 2,160  of  3,500.    Elapsed: 0:11:42.\n",
      "  Batch 2,200  of  3,500.    Elapsed: 0:11:55.\n",
      "  Batch 2,240  of  3,500.    Elapsed: 0:12:08.\n",
      "  Batch 2,280  of  3,500.    Elapsed: 0:12:21.\n",
      "  Batch 2,320  of  3,500.    Elapsed: 0:12:34.\n",
      "  Batch 2,360  of  3,500.    Elapsed: 0:12:47.\n",
      "  Batch 2,400  of  3,500.    Elapsed: 0:13:00.\n",
      "  Batch 2,440  of  3,500.    Elapsed: 0:13:13.\n",
      "  Batch 2,480  of  3,500.    Elapsed: 0:13:26.\n",
      "  Batch 2,520  of  3,500.    Elapsed: 0:13:39.\n",
      "  Batch 2,560  of  3,500.    Elapsed: 0:13:52.\n",
      "  Batch 2,600  of  3,500.    Elapsed: 0:14:05.\n",
      "  Batch 2,640  of  3,500.    Elapsed: 0:14:18.\n",
      "  Batch 2,680  of  3,500.    Elapsed: 0:14:31.\n",
      "  Batch 2,720  of  3,500.    Elapsed: 0:14:44.\n",
      "  Batch 2,760  of  3,500.    Elapsed: 0:14:57.\n",
      "  Batch 2,800  of  3,500.    Elapsed: 0:15:10.\n",
      "  Batch 2,840  of  3,500.    Elapsed: 0:15:23.\n",
      "  Batch 2,880  of  3,500.    Elapsed: 0:15:36.\n",
      "  Batch 2,920  of  3,500.    Elapsed: 0:15:49.\n",
      "  Batch 2,960  of  3,500.    Elapsed: 0:16:01.\n",
      "  Batch 3,000  of  3,500.    Elapsed: 0:16:15.\n",
      "  Batch 3,040  of  3,500.    Elapsed: 0:16:28.\n",
      "  Batch 3,080  of  3,500.    Elapsed: 0:16:41.\n",
      "  Batch 3,120  of  3,500.    Elapsed: 0:16:53.\n",
      "  Batch 3,160  of  3,500.    Elapsed: 0:17:06.\n",
      "  Batch 3,200  of  3,500.    Elapsed: 0:17:20.\n",
      "  Batch 3,240  of  3,500.    Elapsed: 0:17:33.\n",
      "  Batch 3,280  of  3,500.    Elapsed: 0:17:45.\n",
      "  Batch 3,320  of  3,500.    Elapsed: 0:17:58.\n",
      "  Batch 3,360  of  3,500.    Elapsed: 0:18:11.\n",
      "  Batch 3,400  of  3,500.    Elapsed: 0:18:24.\n",
      "  Batch 3,440  of  3,500.    Elapsed: 0:18:37.\n",
      "  Batch 3,480  of  3,500.    Elapsed: 0:18:50.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:18:57\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.40\n",
      "  Validation took: 0:01:16\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:00:44 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('Epoch {:} / {:} '.format(epoch_i + 1, epochs))\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {}  of  {}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
    "\n",
    " \n",
    "        # batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_loader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training took: {:}\".format(training_time))\n",
    "        \n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in valid_loader:\n",
    "    \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        (loss, logits) = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "\n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_loader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(valid_loader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.35042030596124407,\n",
       "  'Valid. Loss': 0.3146270592676786,\n",
       "  'Valid. Accur.': 0.9210666666666657,\n",
       "  'Training Time': '0:19:02',\n",
       "  'Validation Time': '0:01:16'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.17831795548703355,\n",
       "  'Valid. Loss': 0.3476218721763774,\n",
       "  'Valid. Accur.': 0.9322666666666658,\n",
       "  'Training Time': '0:18:58',\n",
       "  'Validation Time': '0:01:16'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': 0.06698844165046258,\n",
       "  'Valid. Loss': 0.4026231278688356,\n",
       "  'Valid. Accur.': 0.9322666666666658,\n",
       "  'Training Time': '0:18:57',\n",
       "  'Validation Time': '0:01:16'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "\n",
    "    outputs = model(b_input_ids, token_type_ids=None, \n",
    "                    attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "total_test_accuracy = 0\n",
    "for pred, label in zip(predictions, true_labels):\n",
    "    total_test_accuracy += accuracy(pred, label)\n",
    "\n",
    "avg_test_accuracy = total_test_accuracy / len(test_loader)\n",
    "print(\"Test accuracy: {0:.2f}\".format(avg_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=tokenizer.model_max_length,\n",
    "                                      pad_to_max_length=True, return_tensors='pt', return_attention_mask=True)\n",
    "    tokenized = tokenized.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokenized['input_ids'], \n",
    "                          token_type_ids=None, \n",
    "                          attention_mask=tokenized['attention_mask'])\n",
    "    \n",
    "    logits = logits[0]\n",
    "    print(torch.sigmoid(logits))\n",
    "    pred = np.argmax(logits.cpu(), axis = 1)\n",
    "    if pred == 1:\n",
    "        print('Positive review!')\n",
    "    else:\n",
    "        print('Negative review!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0181, 0.9910]], device='cuda:0')\n",
      "Positive review!\n"
     ]
    }
   ],
   "source": [
    "pos_rew = 'This film is great'\n",
    "predict_sentiment(pos_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9816, 0.0116]], device='cuda:0')\n",
      "Negative review!\n"
     ]
    }
   ],
   "source": [
    "neg_rew = \"This film is terrible\"\n",
    "predict_sentiment(neg_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
